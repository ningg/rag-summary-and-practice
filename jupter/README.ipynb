{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bccd2a8",
   "metadata": {},
   "source": [
    "# rag-summary-and-practice\n",
    "RAG 技术要点、本地实践\n",
    "\n",
    "## 0.背景\n",
    "\n",
    "最近几周工作上，接触些 RAG 内容，看了点资料；本着`最好的学习是复述`原则，把所有要点，重新梳理下。\n",
    "\n",
    "思路：\n",
    "\n",
    "1.RAG 解决什么问题？\n",
    "2.RAG 核心原理、核心组件\n",
    "3.RAG 高级技术，不同组件的高级用法\n",
    "4.效果评估\n",
    "5.后续发展方向\n",
    "\n",
    "## 1.RAG 解决什么问题\n",
    "\n",
    "LLM 基于大规模数据的预训练，获取的通用知识。对于`私有数据`和`高频更新数据`，LLM 无法及时更新。如果采用 `Fine-Tuning` 监督微调方式，LLM 训练成本也较高，而且无法解决`幻觉`问题。 \n",
    "\n",
    "即，`私有数据`和`高频更新数据`，以及`幻觉`问题，LLM 模型自身解决成本较高，因此，引入 RAG `Retrieval Augmented Generation`。\n",
    "\n",
    "\n",
    "## 2.核心原理\n",
    "\n",
    "RAG 检索增强生成：通过检索`外部数据源`信息，构造`融合上下文`（Context），输入给 LLM，获取更准确的结果。\n",
    "\n",
    "核心环节：\n",
    "\n",
    "a. 索引（indexing）\n",
    "b. 检索（retrieval）\n",
    "c. 生成（generation）\n",
    "\n",
    "\n",
    "下述 RAG 架构图中，出了上面 3 个核心环节，还有：查询优化、路由、查询构造\n",
    "\n",
    "* 查询优化（Query Translation）：查询重写、查询扩展、预查伪文档；\n",
    "* 路由（Routing）：根据查询，判断从哪些数据源，获取信息；\n",
    "* 查询抽取（Query Construction）：从原始 Query 中，抽取 SQL 、 Cypher、metadatas，分别用于 关系数据库、图数据库、向量数据库的查询。\n",
    "\n",
    "![rag_detail_v2](../img/rag-overview.png)\n",
    "\n",
    "\n",
    "开始之前，先在本地安装好 Ollama，并且下载好 embedding model 和 language model。\n",
    "\n",
    "* TODO：增加一个链接.\n",
    "\n",
    "安装依赖：\n",
    "\n",
    "* TODO 增加 python 依赖以及版本？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-ollama langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea0ef1",
   "metadata": {},
   "source": [
    "\n",
    "### 2.1. RAG Oveview\n",
    "\n",
    "完整的 indexing、retrieval、generation 实例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934237f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import OllamaLLM, OllamaEmbeddings\n",
    "\n",
    "#### 1.INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### 2.RETRIEVAL and 3.GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "# Pull a pre-made RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "# Post-processing\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Helper function to remove <think> part in the text\n",
    "def remove_think_tags(text):\n",
    "    \"\"\"remove <think> part in the text\"\"\"\n",
    "    cleaned_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n', '\\n', cleaned_text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    # | remove_think_tags\n",
    ")\n",
    "\n",
    "# Question\n",
    "# Ask a question using the RAG chain\n",
    "response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb616381",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Indexing\n",
    "\n",
    "几个方面：\n",
    "\n",
    "1. Tokenizer：分词，文本会被拆分成 token，映射到词表中 tokenID。\n",
    "2. Embedding：嵌入，将 tokenID 映射到向量空间中，得到 token 的向量表示。\n",
    "3. Chunk：分块，将文本拆分成多个 chunk，每个 chunk 包含多个 token。\n",
    "4. Index：索引，将 chunk 的向量表示存储到向量数据库中。\n",
    "\n",
    "#### 2.2.1.Token\n",
    "\n",
    "更多细节， [Count tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) and [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)\n",
    "\n",
    "> TODO: token 的扩展信息，参考上面链接.\n",
    "\n",
    "查看下面分词得到的 Token："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Documents\n",
    "document = \"My favorite pet is a cat.\"\n",
    "question = \"What kinds of pets do I like?\"\n",
    "\n",
    "# count token num\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokenIDs = encoding.encode(string)\n",
    "\n",
    "    print('tokenIDs: ' + str(tokenIDs))\n",
    "\n",
    "    num_tokens = len(tokenIDs)\n",
    "    return num_tokens\n",
    "\n",
    "# use cl100k_base encoding\n",
    "result = num_tokens_from_string(question, \"cl100k_base\")\n",
    "print('token num: ' + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfd2ce",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.2.2.Embedding\n",
    "\n",
    "[Ollama Embedding](https://python.langchain.com/docs/integrations/text_embedding/ollama/) ，实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e85b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embd = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "result = len(query_result)\n",
    "\n",
    "print('query_result: ' + str(query_result))\n",
    "print('embedding dim: ' + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bfa5c",
   "metadata": {},
   "source": [
    "\n",
    "衡量 2 个 embedding 结果的关联关系，使用 `cosine similarity`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74333e",
   "metadata": {},
   "source": [
    "\n",
    "> TODO: 增加 cosine similarity 物理含义的说明.\n",
    "\n",
    "#### 2.2.3.Chunk\n",
    "\n",
    "LangChain 提供了关联工具：\n",
    "\n",
    "* [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)：加载各类文档数据，并转换为 LangChain 的 Document 标准对象。\n",
    "* [Text Splitters](https://python.langchain.com/api_reference/text_splitters/index.html)：将文本拆分成多个 chunk，每个 chunk 包含多个 token。\n",
    "\n",
    "下面使用 `RecursiveCharacterTextSplitter` 进行分割："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Print splits\n",
    "print(\"Print splits 1:\" + splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c3cda",
   "metadata": {},
   "source": [
    "\n",
    "> RecursiveCharacterTextSplitter: 原理细节，TODO\n",
    "\n",
    "\n",
    "#### 2.2.4.Index\n",
    "\n",
    "有多种向量数据库，下面使用 Chroma 进行演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0de64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675f8ef",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3. Retrieval\n",
    "\n",
    "上面建好了索引，现在进行检索："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606dc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 参数含义\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
    "\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c8318",
   "metadata": {},
   "source": [
    "\n",
    "### 2.4. Generation\n",
    "\n",
    "![](../img/overview-generation.png)\n",
    "\n",
    "> 在 LLM（大语言模型） 相关的 向量检索 / `ANN`（**Approximate Nearest Neighbor**, 近似最近邻） 场景里，HNSW 是一种非常常用的索引结构，含义是： `HNSW` = `Hierarchical Navigable Small World graph`，**分层可导航小世界图**。\n",
    "\n",
    "代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "# Chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8b73b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "也可以使用封装的 prompt 模板，同时，构造完整的 RAG Chain："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c84bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Pull a pre-made RAG prompt from LangChain Hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(\"prompt_hub_rag: \" + str(prompt_hub_rag))\n",
    "\n",
    "# RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad8867",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.高级技术：查询转换（Query Translation）\n",
    "\n",
    "查询转换：将原始查询转换为更适合 LLM 理解的查询。\n",
    "\n",
    "几种常用方法：FIXME\n",
    "\n",
    "* 查询重写 Multi Query：换一种说法，表达查询意图。\n",
    "* 查询扩展：添加更多信息，帮助 LLM 理解查询意图。\n",
    "* 预查伪文档：预先构建一些伪文档，帮助 LLM 理解查询意图。\n",
    "* 多个子查询：依赖 LLM 生成多个子查询，然后分别检索，最后合并结果。\n",
    "\n",
    "\n",
    "构建基础信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650171b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407a9ac",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.查询重写 Multi Query\n",
    "\n",
    "典型的 prompt："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3832de",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are an AI language model assistant. \n",
    "Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database. \n",
    "By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4877263",
   "metadata": {},
   "source": [
    "\n",
    "使用 LLM 重写查询语句（包含 Role、Goal、Constraints），返回多个查询语句，示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | OllamaLLM(model=\"deepseek-r1:8b\") \n",
    "    | StrOutputParser() \n",
    "    | remove_think_tags\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd724e",
   "metadata": {},
   "source": [
    "\n",
    "使用重写得到的 5 个 Query，分别检索，并将关联文档进行`去重`：\n",
    "\n",
    "TODO：dumps、loads 含义？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b6829",
   "metadata": {},
   "source": [
    "\n",
    "使用上面得到的关联文档，输入给 LLM，获取最终答案："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3b1de",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2.查询融合 RAG Fusion\n",
    "\n",
    "**查询融合 RAG Fusion**：将多个查询的关联文档进行融合(**去重**、`Ranking Fusion`**排序**等)，将最相关的文档排在**最前面**，输入给 LLM，获取最终答案。\n",
    "\n",
    "![](../img/rag-fusion.png)\n",
    "\n",
    "下文没有突出 `查询重写`，所以，用了最简单的 prompt 来生成多个`查询`，实际场景中，建议使用 `查询重写`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# the pre-made prompt of hub:\n",
    "# from langchain import hub\n",
    "# prompt = hub.pull(\"langchain-ai/rag-fusion-query-generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc531de",
   "metadata": {},
   "source": [
    "\n",
    "基于上述 prompt，构造 multi query chain："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "    | StrOutputParser() \n",
    "    | remove_think_tags\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140917f",
   "metadata": {},
   "source": [
    "\n",
    "下面是 RAG Fusion 的核心，采用 `RRF`（**Reciprocal Rank Fusion**，倒数排序融合）来融合查询到的文档：\n",
    "\n",
    "> TODO: 增加 RRF 的物理含义的说明，以及局限性。输入参数 k 的作用，取值有什么考量？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18253497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        \n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            \n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            \n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            \n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            # The core of RRF: documents ranked higher (lower rank value) get a larger score\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, \n",
    "    # each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries \n",
    "                    | retriever.map() \n",
    "                    | reciprocal_rank_fusion\n",
    "\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf695b15",
   "metadata": {},
   "source": [
    "\n",
    "下面，编写 RAG Fusion Chain："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2ab3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9bdfe",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3.查询拆解 Query Decomposition\n",
    "\n",
    "**查询拆解**：将原始查询拆分成多个`子查询`，然后`分别检索`，最后`合并`结果。\n",
    "\n",
    "**适用场景**：有些复杂问题，其中包含了多个子问题，**无法在一个步骤中解决**。例如，**What are the main components of an LLM-powered agent, and how do they interact?** 这实际就是 2 个问题。\n",
    "\n",
    "![](../img/query-decomposition.png)\n",
    "\n",
    "\n",
    "实际上，查询拆解为多个子查询后，不同的查询之间，可能存在 2 类关系：前后依赖、相互独立。\n",
    "\n",
    "典型的 prompt (Role、Goal、Constraints)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f1581",
   "metadata": {},
   "source": [
    "\n",
    "查询拆解，获取到多个子查询，实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db708d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")) | remove_think_tags)\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
    "\n",
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3054825e",
   "metadata": {},
   "source": [
    "\n",
    "使用上面得到的 questions，分别检索，并使用 RAG 获取答案，实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser() | remove_think_tags)\n",
    "                                .invoke({\"context\": retrieved_docs, \n",
    "                                        \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, \n",
    "                        prompt_rag, generate_queries_decomposition)\n",
    "\n",
    "\n",
    "# Q+A pairs\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd8a05c",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4.后退查询 Step Back\n",
    "\n",
    "后退查询：将原始查询`后退`一步，重新构造查询，然后检索，获取答案。\n",
    "\n",
    "* 对`原始查询`，进行概念和原则的**抽象化处理**，从而引导更加深入的推理过程；\n",
    "* 一般会**去掉不必要的细节**，从而引导更加深入的推理过程；\n",
    "\n",
    "![](../img/query-step-back-prompt.png)\n",
    "\n",
    "上面示意图，列出了 `Step-back prompt`、`Sub-question`（`Query Decomposition`）、`Re-written`(`Multi-Query`&`RAG Fusion`) 3 个环节的位置。\n",
    "\n",
    "采用 小样本学习（few-shot），来引导 LLM 进行后退查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# step-back chain\n",
    "generate_queries_step_back = prompt \n",
    "    | OllamaLLM(model=\"deepseek-r1:8b\") \n",
    "    | StrOutputParser()\n",
    "    | remove_think_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed99294",
   "metadata": {},
   "source": [
    "\n",
    "使用上面得到的 query，进行检索，并使用 RAG 获取答案，实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb67c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | OllamaLLM(model=\"deepseek-r1:8b\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e254ec2",
   "metadata": {},
   "source": [
    "\n",
    "### 3.5.假设性文档嵌入 HYDE\n",
    "\n",
    "![](../img/query-HYDE.png)\n",
    "\n",
    "查询相关文档时，最主要的问题是：查询用语 跟 文档内容之间，存在术语不统一、词表不一致的问题。\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** ：让 llm 先生成一份书面的回答（`假设性回答`），并以此作为`查询嵌入`后，获取对应关联文档；再用 `原始查询` + 关联文档，获取最终生成的内容。\n",
    "\n",
    "* `ori-Query` -> LLM -> **Hypothetical Answer**(`hypo-Query`) -> Retrieval -> `Documents`\n",
    "* `ori-Query` + `Documents` -> LLM -> Answer\n",
    "\n",
    "\n",
    "采用 HYDE（`Hypothetical Document Embeddings` 假设性回答文档嵌入） 实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5af739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde \n",
    "    | OllamaLLM(model=\"deepseek-r1:8b\") \n",
    "    | StrOutputParser() \n",
    "    | remove_think_tags\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52570a",
   "metadata": {},
   "source": [
    "\n",
    "使用上面得到的假设性文档，进行检索，并使用 RAG 获取答案，实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retrieved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aaa507",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "关联资料\n",
    "\n",
    "* [rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch)\n",
    "* [rag-ecosystem](https://github.com/FareedKhan-dev/rag-ecosystem)\n",
    "* [检索增强生成 (RAG) 方法](https://www.promptingguide.ai/zh/research/rag)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
